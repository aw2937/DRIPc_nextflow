#!/usr/bin/env nextflow
nextflow.enable.dsl=2

// Define Default Parameters
params.input_dir = "data/fastq" // Directory where FASTQ files matching globs are located
params.samplesheet = "samples.csv"
params.outdir = "results"
params.skip_fastqc = false
params.skip_multiqc = false
params.skip_macs3 = false
params.skip_dropa = true // Often resource/time intensive or needs specific setup
params.skip_chedin = true // Custom method, often needs specific setup

// Parameters for hg19 analysis
params.bowtie2_index_hg19 = "ref_genome/hg19/hg19" // Bowtie2 index prefix
params.genome_fai_hg19 = "ref_genome/hg19/hg19.fa.fai" // Genome fasta index
params.macs_gsize_hg19 = "hs"
params.dropa_ref_hg19 = "GeneReference/hg19_RefSeq/"
params.dropa_gsize_hg19 = "GeneReference/hg19.genome"

// Paths to custom scripts and models - place these in a 'bin' dir or specify full paths
params.dropa_script = "${projectDir}/bin/DROPA_v1.0.0.py"
params.normalize_script = "${projectDir}/bin/normalize.pl"
params.wig2fa_script = "${projectDir}/bin/wig2fa.pl"
params.dripc_hmm_model = "${projectDir}/bin/DRIPc.hmm"

log.info """
         D R I P - S E Q   P I P E L I N E
         =================================
         Input Samplesheet: ${params.samplesheet}
         Input Directory  : ${params.input_dir}
         Output Directory : ${params.outdir}
         Genome (hg19)    : Bowtie2 Index: ${params.bowtie2_index_hg19}, FAI: ${params.genome_fai_hg19}
         ---
         Run FastQC       : ${!params.skip_fastqc}
         Run MultiQC      : ${!params.skip_multiqc}
         Run MACS3        : ${!params.skip_macs3}
         Run DROPA        : ${!params.skip_dropa}
         Run Chedin       : ${!params.skip_chedin}
         Custom Scripts:
           DROPA script   : ${params.dropa_script}
           Normalize.pl   : ${params.normalize_script}
           Wig2Fa.pl      : ${params.wig2fa_script}
           DRIPc HMM      : ${params.dripc_hmm_model}
         """.stripIndent()

// Validate custom script paths
def check_file(path, name) {
    if (!file(path).exists()) {
        exit 1, "${name} not found at ${path}. Please provide a valid path."
    }
    return file(path)
}

dropa_script_file = check_file(params.dropa_script, "DROPA script")
normalize_script_file = check_file(params.normalize_script, "normalize.pl script")
wig2fa_script_file = check_file(params.wig2fa_script, "wig2fa.pl script")
dripc_hmm_file = check_file(params.dripc_hmm_model, "DRIPc HMM model")


// --- Channel for input samples ---
Channel
    .fromPath(params.samplesheet)
    .splitCsv(header:true)
    .map { row ->
        def sample_id = row.sample
        def fastq_glob_pattern = row.fastq_glob
        def fastq_files = file("${params.input_dir}/${fastq_glob_pattern}").toList()
        if (fastq_files.isEmpty()) {
            log.warn "WARNING: No files found for sample ${sample_id} with glob ${params.input_dir}/${fastq_glob_pattern}"
            return null // Or handle error differently
        }
        // Sort files to ensure deterministic cat order
        fastq_files.sort()
        return tuple(sample_id, fastq_files)
    }
    .filter { it != null } // Remove samples for which no files were found
    .set { ch_raw_fastqs }


// --- Workflow Definition ---
workflow {
    MERGE_FASTQS(ch_raw_fastqs)
    ch_merged_fastqs = MERGE_FASTQS.out

    if (!params.skip_fastqc) {
        FASTQC_RAW(ch_merged_fastqs)
    }

    TRIM_GALORE(ch_merged_fastqs)
    ch_trimmed_fastqs = TRIM_GALORE.out.trimmed_fq

    if (!params.skip_fastqc) {
        FASTQC_TRIMMED(ch_trimmed_fastqs)
    }

    BOWTIE2_ALIGN_PROCESS(
        ch_trimmed_fastqs,
        file(params.bowtie2_index_hg19 + ".1.bt2"), // Pass one of the index files to stage the whole set
        file(params.genome_fai_hg19)
    )
    ch_aligned_bams = BOWTIE2_ALIGN_PROCESS.out.bam_indexed // tuple(sample_id, markdup_bam, markdup_bai)

    GET_COVERAGE(
        ch_aligned_bams.map { id, bam, bai -> tuple(id, bam) }, // only need bam
        file(params.genome_fai_hg19)
    )

    SPLIT_BAM(ch_aligned_bams.map { id, bam, bai -> tuple(id, bam) }) // only need bam
    ch_split_bams_fwd = SPLIT_BAM.out.fwd_bam_indexed // tuple(id, fwd_bam, fwd_bai)
    ch_split_bams_rev = SPLIT_BAM.out.rev_bam_indexed // tuple(id, rev_bam, rev_bai)

    if (!params.skip_macs3) {
        MACS3_PEAKCALLING(
            ch_split_bams_fwd.map { id, bam, bai -> tuple(id, bam) }, // only need bam
            params.macs_gsize_hg19
        )
        if (!params.skip_dropa && !MACS3_PEAKCALLING.out.broadpeaks.isEmpty()) {
            DROPA_ANNOTATION(
                MACS3_PEAKCALLING.out.broadpeaks,
                dropa_script_file,
                file(params.dropa_ref_hg19),
                file(params.dropa_gsize_hg19)
            )
        }
    }

    if (!params.skip_chedin) {
        CHEDIN_SAMTOOLS_DEPTH(
            ch_split_bams_fwd.map { id, bam, bai -> tuple(id, bam, "fwd") }
                .mix(ch_split_bams_rev.map { id, bam, bai -> tuple(id, bam, "rev") })
        )
        CHEDIN_NORMALIZE_WIG(CHEDIN_SAMTOOLS_DEPTH.out.wig, normalize_script_file)
        CHEDIN_WIG2FA(CHEDIN_NORMALIZE_WIG.out.norm_wig, wig2fa_script_file)
        CHEDIN_STOCHHMM(CHEDIN_WIG2FA.out.custom_fa, dripc_hmm_file)
    }

    if (!params.skip_multiqc && !params.skip_fastqc) {
        ch_multiqc_files = Channel.empty()
        ch_multiqc_files = ch_multiqc_files.mix(FASTQC_RAW.out.zip)
        ch_multiqc_files = ch_multiqc_files.mix(FASTQC_TRIMMED.out.zip)
        // Add other MultiQC compatible files if generated (e.g., trim_galore reports)
        ch_multiqc_files = ch_multiqc_files.mix(TRIM_GALORE.out.report)

        MULTIQC(ch_multiqc_files.collect())
    }
}

// --- Process Definitions ---

process MERGE_FASTQS {
    tag "$sample_id"
    publishDir "${params.outdir}/01_merged_fastq", mode: 'copy', pattern: "*.fastq.gz"

    input:
    tuple val(sample_id), path(fastqs)

    output:
    tuple val(sample_id), path("${sample_id}_merged.fastq.gz"), emit: merged_fastq

    script:
    """
    cat ${fastqs.join(' ')} > ${sample_id}_merged.fastq.gz
    """
}

process FASTQC_RAW {
    tag "$sample_id"
    publishDir "${params.outdir}/02_fastqc_raw", mode: 'copy'
    conda "bioconda::fastqc=0.12.1"

    when: !params.skip_fastqc

    input:
    tuple val(sample_id), path(merged_fastq)

    output:
    path("*.html"), emit: html
    path("*.zip"), emit: zip

    script:
    """
    fastqc -o . $merged_fastq
    """
}

process TRIM_GALORE {
    tag "$sample_id"
    publishDir "${params.outdir}/03_trimmed_fastq", mode: 'copy', pattern: "*_trimmed.fq.gz"
    publishDir "${params.outdir}/03_trimmed_fastq/reports", mode: 'copy', pattern: "*trimming_report.txt"
    conda "bioconda::trim-galore=0.6.10 bioconda::cutadapt=4.1" // TrimGalore needs cutadapt

    input:
    tuple val(sample_id), path(merged_fastq)

    output:
    tuple val(sample_id), path("${sample_id}_merged_trimmed.fq.gz"), emit: trimmed_fq
    path("*_trimming_report.txt"), emit: report

    script:
    def original_name = merged_fastq.getBaseName() // e.g., EH-1_merged
    """
    trim_galore --gzip -o . $merged_fastq
    mv ${original_name}_trimmed.fq.gz ${sample_id}_merged_trimmed.fq.gz // Ensure consistent naming
    mv ${original_name}_trimming_report.txt ${sample_id}_merged_trimming_report.txt
    """
}

process FASTQC_TRIMMED {
    tag "$sample_id"
    publishDir "${params.outdir}/04_fastqc_trimmed", mode: 'copy'
    conda "bioconda::fastqc=0.12.1"

    when: !params.skip_fastqc

    input:
    tuple val(sample_id), path(trimmed_fastq)

    output:
    path("*.html"), emit: html
    path("*.zip"), emit: zip

    script:
    """
    fastqc -o . $trimmed_fastq
    """
}

process BOWTIE2_ALIGN_PROCESS {
    tag "$sample_id"
    publishDir "${params.outdir}/05_bam_files", mode: 'copy', pattern: "*.{bam,bai}"
    conda "bioconda::bowtie2=2.5.1 bioconda::samtools=1.17"

    input:
    tuple val(sample_id), path(trimmed_fastq)
    path bowtie2_index // Pass e.g. hg19.1.bt2, Nextflow stages all hg19.*.bt2
    path genome_fai

    output:
    tuple val(sample_id), path("${sample_id}_markdup.bam"), path("${sample_id}_markdup.bam.bai"), emit: bam_indexed

    script:
    def bt2_idx_prefix = bowtie2_index.toString().replaceAll(/\.1\.bt2$/, "")
    """
    bowtie2 -p ${task.cpus} -x $bt2_idx_prefix -U $trimmed_fastq | \\
        samtools view -bS -t $genome_fai - > ${sample_id}_aligned.bam

    samtools sort -@ ${task.cpus} -n ${sample_id}_aligned.bam -o ${sample_id}_nsorted.bam
    samtools fixmate -@ ${task.cpus} -m ${sample_id}_nsorted.bam ${sample_id}_fixmate.bam
    samtools sort -@ ${task.cpus} ${sample_id}_fixmate.bam -o ${sample_id}_psorted.bam
    samtools markdup -@ ${task.cpus} -r -s ${sample_id}_psorted.bam ${sample_id}_markdup.bam
    samtools index -@ ${task.cpus} ${sample_id}_markdup.bam
    """
}

process GET_COVERAGE {
    tag "$sample_id"
    publishDir "${params.outdir}/06_coverage_bigwig", mode: 'copy', pattern: "*.bigWig"
    conda "bioconda::samtools=1.17 bioconda::bedtools=2.30.0 ucsc-wigtobigwig=377"
    // Note: wigToBigWig might be tricky with conda, might need specific channel or manual install path

    input:
    tuple val(sample_id), path(markdup_bam)
    path genome_fai

    output:
    tuple val(sample_id), path("${sample_id}_pos.bigWig"), emit: pos_bw
    tuple val(sample_id), path("${sample_id}_neg.bigWig"), emit: neg_bw

    script:
    """
    # Positive strand
    samtools view -@ ${task.cpus} $markdup_bam | awk '\$2 ~ /^(16|83|163)\$/{print}' | \\
        samtools view -@ ${task.cpus} -bS -t $genome_fai - | \\
        bedtools genomecov -ibam - -split -bg -g $genome_fai > ${sample_id}_pos.bedGraph
    gzip ${sample_id}_pos.bedGraph
    wigToBigWig -clip ${sample_id}_pos.bedGraph.gz $genome_fai ${sample_id}_pos.bigWig

    # Negative strand
    samtools view -@ ${task.cpus} $markdup_bam | awk '\$2 ~ /^(0|99|147)\$/{print}' | \\
        samtools view -@ ${task.cpus} -bS -t $genome_fai - | \\
        bedtools genomecov -ibam - -split -bg -g $genome_fai > ${sample_id}_neg.bedGraph
    gzip ${sample_id}_neg.bedGraph
    wigToBigWig -clip ${sample_id}_neg.bedGraph.gz $genome_fai ${sample_id}_neg.bigWig
    """
}

process SPLIT_BAM {
    tag "$sample_id"
    publishDir "${params.outdir}/07_split_bams", mode: 'copy', pattern: "*.{bam,bai}"
    conda "bioconda::samtools=1.17"

    input:
    tuple val(sample_id), path(markdup_bam)

    output:
    tuple val(sample_id), path("${sample_id}_fwd.bam"), path("${sample_id}_fwd.bam.bai"), emit: fwd_bam_indexed
    tuple val(sample_id), path("${sample_id}_rev.bam"), path("${sample_id}_rev.bam.bai"), emit: rev_bam_indexed

    script:
    """
    samtools view -@ ${task.cpus} -F 16 -o ${sample_id}_fwd.bam $markdup_bam
    samtools index -@ ${task.cpus} ${sample_id}_fwd.bam

    samtools view -@ ${task.cpus} -f 16 -o ${sample_id}_rev.bam $markdup_bam
    samtools index -@ ${task.cpus} ${sample_id}_rev.bam
    """
}

process MACS3_PEAKCALLING {
    tag "$sample_id"
    publishDir "${params.outdir}/08_macs3_peaks/${sample_id}", mode: 'copy'
    conda "bioconda::macs3=3.0.0"

    when: !params.skip_macs3

    input:
    tuple val(sample_id), path(fwd_bam)
    val macs_gsize

    output:
    tuple val(sample_id), path("${sample_id}_peaks.broadPeak"), emit: broadpeaks, optional: true // Make optional if MACS3 might not produce output
    path("*"), emit: all_files // All MACS3 output files

    script:
    """
    macs3 callpeak -t $fwd_bam \\
        --broad --nomodel --extsize 150 \\
        -g $macs_gsize \\
        --outdir . -n $sample_id
    """
}

process DROPA_ANNOTATION {
    tag "$sample_id"
    publishDir "${params.outdir}/09_dropa_annotation/${sample_id}", mode: 'copy'
    // Conda for DROPA might be tricky as it's a custom Python3 script.
    // Ensure Python3 and its dependencies (e.g., pandas, numpy, scipy) are available.
    // You might need a custom environment file.
    // For now, assuming a base python3 environment.
    conda "conda-forge::python=3.9 bioconda::pandas bioconda::numpy bioconda::scipy"

    when: !params.skip_dropa

    input:
    tuple val(sample_id), path(broad_peak_file)
    path dropa_script
    path dropa_ref_dir
    path dropa_gsize_file

    output:
    tuple val(sample_id), path("${sample_id}_annotated_peaks"), emit: annotated_dir // DROPA output might be a directory or specific files
    //path("${sample_id}_annotated_peaks/*") // Example if it creates multiple files in a dir

    script:
    // Original script uses `out="${entry##*/}"` which is the peak filename.
    // DROPA's -o seems to be an output *prefix* or *directory*. Let's assume directory.
    """
    mkdir ${sample_id}_annotated_peaks
    python3 $dropa_script \\
        -ref $dropa_ref_dir \\
        -o ${sample_id}_annotated_peaks/${sample_id}_annotated \\
        -shuffle 2 \\
        -gsize $dropa_gsize_file \\
        $broad_peak_file
    """
}

process CHEDIN_SAMTOOLS_DEPTH {
    tag "$sample_id ($strand)"
    publishDir "${params.outdir}/10_chedin_method/wigs", mode: 'copy', pattern: "*.wig"
    conda "bioconda::samtools=1.17 perl-base"

    when: !params.skip_chedin

    input:
    tuple val(sample_id), path(strand_bam), val(strand) // strand is "fwd" or "rev"

    output:
    tuple val(sample_id), val(strand), path("${sample_id}_${strand}.wig"), emit: wig

    script:
    """
    samtools depth $strand_bam | \\
        perl -ne 'BEGIN{ print "track type=wiggle_0 name=${sample_id}_${strand} description=${sample_id}_${strand}\\n"}; (\$c, \$start, \$depth) = split; if (\$c ne \$lastC) { print "variableStep chrom=\$c span=10\\n"; };\$lastC=\$c; next unless \$. % 10 ==0;print "\$start\\t\$depth\\n" unless \$depth<3' > ${sample_id}_${strand}.wig
    """
}

process CHEDIN_NORMALIZE_WIG {
    tag "$sample_id ($strand)"
    publishDir "${params.outdir}/10_chedin_method/normalized_wigs", mode: 'copy', pattern: "*.wig"
    conda "perl-base" // Assuming normalize.pl is a simple perl script

    when: !params.skip_chedin

    input:
    tuple val(sample_id), val(strand), path(input_wig)
    path normalize_script

    output:
    tuple val(sample_id), val(strand), path("${sample_id}_normalized_${strand}.wig"), emit: norm_wig

    script:
    """
    perl $normalize_script $input_wig ${sample_id}_normalized_${strand}.wig
    """
}

process CHEDIN_WIG2FA {
    tag "$sample_id ($strand)"
    publishDir "${params.outdir}/10_chedin_method/custom_fa", mode: 'copy', pattern: "*.customfa"
    conda "perl-base" // Assuming wig2fa.pl is a simple perl script

    when: !params.skip_chedin

    input:
    tuple val(sample_id), val(strand), path(normalized_wig)
    path wig2fa_script

    output:
    tuple val(sample_id), val(strand), path("${sample_id}_${strand}.customfa"), emit: custom_fa

    script:
    """
    perl $wig2fa_script -i $normalized_wig -o ${sample_id}_${strand}.customfa
    """
}

process CHEDIN_STOCHHMM {
    tag "$sample_id ($strand)"
    publishDir "${params.outdir}/10_chedin_method/stochhmm_peaks", mode: 'copy', pattern: "*.peaks"
    // Conda for stochhmm might require a custom build or specific channel.
    // For now, assuming it's in PATH or you define a container/env.
    // If stochhmm is not available via common channels, you might need to use:
    // container 'your_repo/stochhmm_container:tag'
    // Or install it into a conda env and activate that env.
    // For this example, let's assume it's in the path for simplicity.
    // conda "..." // Add stochhmm if available

    when: !params.skip_chedin

    input:
    tuple val(sample_id), val(strand), path(custom_fa)
    path dripc_hmm_model

    output:
    tuple val(sample_id), val(strand), path("${sample_id}_${strand}.peaks"), emit: peaks_gff

    script:
    """
    stochhmm -seq $custom_fa -model $dripc_hmm_model -posterior -threshold 0.9 -gff > ${sample_id}_${strand}.peaks
    """
}


process MULTIQC {
    publishDir "${params.outdir}/multiqc", mode: 'copy'
    conda "bioconda::multiqc=1.19"

    when: !params.skip_multiqc

    input:
    path '*' // Collects all files passed to it

    output:
    path "multiqc_report.html", emit: report
    path "multiqc_data", emit: data

    script:
    """
    multiqc .
    """
}
